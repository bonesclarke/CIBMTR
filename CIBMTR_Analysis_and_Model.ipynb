{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip download lifelines\n",
    "#%pip install input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "#%pip install input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "#%pip install input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "#%pip install input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "#%pip install input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from scipy.stats import rankdata \n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import catboost as cb\n",
    "\n",
    "from metric import score\n",
    "import optuna\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set analysis output directory\n",
    "def create_output_directory(output_path):\n",
    "    \"\"\"Create the output directory if it doesn't exist and set plotting style.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    return output_path\n",
    "\n",
    "output_path = 'working/analysis'\n",
    "create_output_directory(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "test = pd.read_csv(\"input/data/test.csv\")\n",
    "print(\"Test shape:\", test.shape )\n",
    "\n",
    "train = pd.read_csv(\"input/data/train.csv\")\n",
    "print(\"Train shape:\",train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train targets\n",
    "plt.hist(train.loc[train.efs==1,\"efs_time\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train.loc[train.efs==0,\"efs_time\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Time of Observation, efs_time\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Times of Observation. Either time to event, or time observed without event.\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_path}times_of_observation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values heatmap\n",
    "def plot_missing_values_heatmap(df, output_path):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/missing_values_heatmap.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_missing_values_heatmap(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value percentages\n",
    "def plot_missing_values_bars(df, output_path):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    missing_percentages = (df.isnull().sum() / len(df) * 100).sort_values(ascending=True)\n",
    "    sns.barplot(x=missing_percentages.values, y=missing_percentages.index)\n",
    "    plt.title('Percentage of Missing Values by Column')\n",
    "    plt.xlabel('Percentage Missing')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/missing_values_percentage.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_missing_values_bars(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical distributions\n",
    "def plot_numerical_distributions(df, output_path):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    \n",
    "    # Create progress bar for numerical distributions\n",
    "    for col in tqdm(numerical_cols, desc=\"Creating distribution plots\"):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create subplot with histogram and kde\n",
    "        sns.histplot(data=df, x=col, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add statistical annotations\n",
    "        stats_text = f'Mean: {df[col].mean():.2f}\\n'\n",
    "        stats_text += f'Median: {df[col].median():.2f}\\n'\n",
    "        stats_text += f'Std: {df[col].std():.2f}'\n",
    "        plt.text(0.95, 0.95, stats_text,\n",
    "                transform=plt.gca().transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/distribution_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "plot_numerical_distributions(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "def plot_correlation_matrix(df, output_path):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        plt.figure(figsize=(24, 16))\n",
    "        correlation_matrix = df[numerical_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/correlation_matrix.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "plot_correlation_matrix(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical distributions\n",
    "def plot_categorical_distributions(df, output_path):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    for col in tqdm(categorical_cols, desc=\"Creating categorical plots\"):\n",
    "        if df[col].nunique() < 30:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            value_counts = df[col].value_counts()\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/categorical_{col}.png')\n",
    "            plt.close()\n",
    "\n",
    "plot_categorical_distributions(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train.columns\n",
    "print(f\"Number of Features: {len(FEATURES)} FEATURES: {FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"Missing\")\n",
    "        test[c] = test[c].fillna(\"Missing\")\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = KNNImputer(n_neighbors=101)\n",
    "#numeric_cols = train[FEATURES].select_dtypes(include=['int32', 'float32', 'float64', 'int64']).columns\n",
    "#imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "#train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "#for col in train.columns:\n",
    "#    if col not in numeric_cols:\n",
    "#        train_imputed[col] = train[col]\n",
    "        \n",
    "#train = train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def feature_engineering(df):   \n",
    "    # Create age bins in 5-year intervals\n",
    "    median_age = df['age_at_hct'].median()\n",
    "    df['age_at_hct'] = df['age_at_hct'].replace([np.inf, -np.inf, '', None], np.nan).fillna(median_age)\n",
    "    df['age_bin'] = pd.cut(df['age_at_hct'], \n",
    "                          bins=range(0, 95, 5),  # From 0 to 75 in steps of 5\n",
    "                          labels=[f'{i}-{i+4}' for i in range(0, 90, 5)],\n",
    "                          include_lowest=True)\n",
    "    \n",
    "    # commorbity by age at hct\n",
    "    df['comorbidity_age'] = df['comorbidity_score'] / (df['age_at_hct'])\n",
    "    \n",
    "    df['age_bin_race'] = (df['age_bin'].astype(str) + '_' + \n",
    "                      df['race_group'].astype(str)).astype('category')\n",
    "\n",
    "    # age x cyto\n",
    "    df['cyto_age'] = (df['cyto_score_detail'].astype(str) + '_' + df['age_at_hct'].astype(str)).astype('category')\n",
    "\n",
    "    # Concatenate graft_type and prod_type\n",
    "    df['graft_prod'] = (df['graft_type'].astype(str) + '_' + \n",
    "                    df['prod_type'].astype(str)).astype('category')\n",
    "    \n",
    "    # Concatenate age bin and pulm severe\n",
    "    df['age_bin_pulm_severe'] = (df['age_bin'].astype(str) + '_' + \n",
    "                    df['pulm_severe'].astype(str)).astype('category')\n",
    "    \n",
    "    # Concatenate age bin, race_group, and dri score\n",
    "    df['age_bin_dri'] = (df['age_bin'].astype(str) + '_' + \n",
    "                    df['dri_score'].astype(str)).astype('category')\n",
    "\n",
    "    # hla high mean\n",
    "    df['hla_high_res_mean'] = df[['hla_high_res_8', 'hla_high_res_10', 'hla_high_res_6']].mean(axis=1)\n",
    "\n",
    "    # hla low mean\n",
    "    df['hla_low_res_mean'] = df[['hla_low_res_8', 'hla_low_res_10', 'hla_low_res_6']].mean(axis=1)\n",
    "\n",
    "    # ration of hla high and hla low\n",
    "    df['hla_ratio_res_highlow'] = (df['hla_high_res_mean'])/(df['hla_low_res_mean']+1)\n",
    "\n",
    "    # ration of hla low and hla high\n",
    "    df['hla_ratio_res_lowhigh'] = (df['hla_low_res_mean'])/(df['hla_high_res_mean']+1)\n",
    "\n",
    "    # age functions\n",
    "    df['donor_by_age_at_hct'] = (df['donor_age']/df['age_at_hct'])\n",
    "    df['comorbidity_score_by_age_at_hct'] = (df['comorbidity_score']/df['age_at_hct'])\n",
    "    \n",
    "    # match drb\n",
    "    df['hla_match_drb1_mean'] = df[['hla_match_drb1_high', 'hla_match_drb1_low']].mean(axis=1)\n",
    "\n",
    "    # match dqb\n",
    "    df['hla_match_dqb1_mean'] = df[['hla_match_dqb1_high', 'hla_match_dqb1_low']].mean(axis=1)\n",
    "\n",
    "    # additional ratios\n",
    "    df['hla_high_low_ratio'] = (df['hla_high_res_mean']) / (df['hla_low_res_mean'] + 1)\n",
    "    df['drb1_dqb1_ratio'] = (df['hla_match_drb1_mean']) / (df['hla_match_dqb1_mean'] + 1)\n",
    "\n",
    "    # difference in features\n",
    "    df['high_low_diff'] = df['hla_high_res_mean'] - df['hla_low_res_mean']\n",
    "    df['drb1_dqb1_diff'] = df['hla_match_drb1_mean'] - df['hla_match_dqb1_mean']\n",
    "\n",
    "    # statistical aggregations\n",
    "    df['hla_mean'] = df[['hla_high_res_mean', 'hla_low_res_mean', 'hla_match_drb1_mean', 'hla_match_dqb1_mean']].mean(axis=1)\n",
    "    df['hla_std'] = df[['hla_high_res_mean', 'hla_low_res_mean', 'hla_match_drb1_mean', 'hla_match_dqb1_mean']].std(axis=1)\n",
    "    df['hla_max'] = df[['hla_high_res_mean', 'hla_low_res_mean', 'hla_match_drb1_mean', 'hla_match_dqb1_mean']].max(axis=1)\n",
    "    df['hla_min'] = df[['hla_high_res_mean', 'hla_low_res_mean', 'hla_match_drb1_mean', 'hla_match_dqb1_mean']].min(axis=1)\n",
    "\n",
    "    # interaction terms\n",
    "    df['drb1_high_interaction'] = df['hla_match_drb1_mean'] * df['hla_high_res_mean']\n",
    "    df['dqb1_low_interaction'] = df['hla_match_dqb1_mean'] * df['hla_low_res_mean']\n",
    "\n",
    "    # with or without TBI\n",
    "    df['with_tbi'] = np.where((df['tbi_status']).astype(str) == 'No TBI', 'no', 'yes')\n",
    "\n",
    "    # Create donor sex feature\n",
    "    df['sex_donor'] = (df['sex_match']).astype(str).str[0]\n",
    "\n",
    "    # Create recipient sex feature\n",
    "    df['sex_recipient'] = (df['sex_match']).astype(str).str[2]\n",
    "\n",
    "    # Main drug presence\n",
    "    df['has_FK'] = (df['gvhd_proph']).astype(str).str.contains('FK', na=False).astype(int)\n",
    "    df['has_MMF'] = (df['gvhd_proph']).astype(str).str.contains('MMF', na=False).astype(int)\n",
    "    df['has_MTX'] = (df['gvhd_proph']).astype(str).str.contains('MTX', na=False).astype(int)\n",
    "    df['has_CSA'] = (df['gvhd_proph']).astype(str).str.contains('CSA', na=False).astype(int)\n",
    "    df['has_cyclophosphamide'] = (df['gvhd_proph']).astype(str).str.contains('Cyclophosphamide', na=False).astype(int)\n",
    "\n",
    "    # Check for combination therapy\n",
    "    df['has_combination'] = (df['gvhd_proph']).astype(str).str.contains('\\+', na=False).astype(int)\n",
    "\n",
    "    # Count number of agents (approximate by counting '+' signs)\n",
    "    df['n_agents'] = (df['gvhd_proph']).astype(str).str.count('\\+').add(1)\n",
    "\n",
    "    # Complex vs Simple regimen\n",
    "    df['is_complex'] = (df['gvhd_proph']).astype(str).str.contains('others', na=False).astype(int)\n",
    "\n",
    "    # Depletion-based therapy\n",
    "    df['is_depletion_based'] = (df['gvhd_proph']).astype(str).str.contains('TDEPLETION|CDselect', na=False).astype(int)\n",
    "\n",
    "    # Monotherapy flag\n",
    "    df['is_monotherapy'] = (df['gvhd_proph']).astype(str).str.contains('alone', na=False).astype(int)\n",
    "\n",
    "    # No prophylaxis flag\n",
    "    df['no_prophylaxis'] = ((df['gvhd_proph']).astype(str) == 'No GvHD Prophylaxis').astype(int)\n",
    "\n",
    "    def get_primary_agent(x):\n",
    "        if pd.isna(x):\n",
    "            return 'Unknown'\n",
    "        elif 'FK' in x:\n",
    "            return 'FK-based'\n",
    "        elif 'CSA' in x:\n",
    "            return 'CSA-based'\n",
    "        elif 'Cyclophosphamide' in x:\n",
    "            return 'Cyclophosphamide-based'\n",
    "        elif 'TDEPLETION' in x or 'CDselect' in x:\n",
    "            return 'Depletion-based'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "    df['primary_agent'] = (df['gvhd_proph']).astype(str).apply(get_primary_agent)\n",
    "\n",
    "    # Standard vs Alternative approach\n",
    "    df['is_standard_approach'] = (df['gvhd_proph']).astype(str).str.contains('FK\\+ MMF|FK\\+ MTX|CSA \\+ MTX', na=False).astype(int)\n",
    "\n",
    "    # Experimental/Other\n",
    "    df['is_experimental'] = ((df['gvhd_proph']).astype(str) == 'Other GVHD Prophylaxis').astype(int)\n",
    "\n",
    "    # Combine with other relevant features\n",
    "    df['FK_MMF_interaction'] = df['has_FK'] * df['has_MMF']\n",
    "    df['CSA_MTX_interaction'] = df['has_CSA'] * df['has_MTX']\n",
    "\n",
    "    # karnofsky score\n",
    "    median_karnofsky = df['karnofsky_score'].median()\n",
    "    df['karnofsky_score'] = df['karnofsky_score'].replace([np.inf, -np.inf, '', None], np.nan).fillna(median_karnofsky)\n",
    "    df['karnofsky_donor_comorbidity_age'] = (df['donor_by_age_at_hct']+df['comorbidity_score_by_age_at_hct'])/(df['karnofsky_score'] + 1)\n",
    "    df['karnofsky_has_csa'] = df['has_CSA'] / (df['karnofsky_score'] + 1)\n",
    "    df['karnofsky_is_depletion_based'] = df['is_depletion_based'] / (df['karnofsky_score'] + 1)\n",
    "    df['karnofsky_is_monotherapy'] = df['is_monotherapy'] / (df['karnofsky_score'] + 1)\n",
    "    df['karnofsky_age_at_hct'] = df['karnofsky_score'] * df['age_at_hct']\n",
    "    df['karnofsky_comorbidity_score'] = df['karnofsky_score'] / (df['comorbidity_score'] + 1)\n",
    "    df['comorbidity_score_karnofsky'] = df['comorbidity_score'] / (df['karnofsky_score'] + 1)\n",
    "    df['graft_prod'] = df['graft_prod'].astype('category')\n",
    "\n",
    "    # vivo\n",
    "    df['in_vivo_tcd'].fillna(\"Missing\")\n",
    "    df['prim_disease_hct'].fillna(\"Missing\")\n",
    "    df['comorbidity_score'].fillna(\"Missing\")\n",
    "    median_comorbidity = df['comorbidity_score'].median()\n",
    "    df['comorbidity_score'] = df['comorbidity_score'].replace([np.inf, -np.inf, '', None], np.nan).fillna(median_comorbidity)\n",
    "    df['vivo_age_bin'] = (df['age_bin'].astype(str) + '_' + \n",
    "                    df['in_vivo_tcd'].astype(str)).astype('category')\n",
    "    df['vivo_comorbidity'] = (df['comorbidity_score'].astype(str) + '_' + \n",
    "                    df['in_vivo_tcd'].astype(str)).astype('category')\n",
    "    df['vivo_prim_disease'] = (df['prim_disease_hct'].astype(str) + '_' + \n",
    "                    df['in_vivo_tcd'].astype(str)).astype('category') \n",
    "    \n",
    "    # dri\n",
    "    # Map risk levels to numeric scores\n",
    "    risk_map = {\n",
    "        'Low': 1,\n",
    "        'Intermediate': 2,\n",
    "        'High': 3,\n",
    "        'Very high': 4,\n",
    "        'N/A - non-malignant indication': 0,\n",
    "        'N/A - pediatric': 0,\n",
    "        'N/A - disease not classifiable': 0,\n",
    "        'TBD cytogenetics': np.nan,\n",
    "        'Missing disease status': np.nan\n",
    "    }\n",
    "    \n",
    "    # Create numeric DRI score\n",
    "    df['dri_numeric'] = df['dri_score'].map(risk_map)\n",
    "\n",
    "    # Fill missing values with median\n",
    "    median_dri = df['dri_numeric'].median()\n",
    "    df['dri_numeric'] = df['dri_numeric'].fillna(median_dri)\n",
    "\n",
    "    # Create binary features for risk levels\n",
    "    df['is_high_risk'] = df['dri_score'].isin(['High', 'Very high']).astype(int)\n",
    "    df['is_standard_risk'] = df['dri_score'].isin(['Low', 'Intermediate']).astype(int)\n",
    "    df['is_special_case'] = df['dri_score'].str.contains('N/A|TBD|Missing', na=False).astype(int)\n",
    "\n",
    "    # Combine with other relevant features\n",
    "    df['dri_age'] = df['dri_numeric'] * df['age_at_hct']\n",
    "    df['dri_comorbidity'] = df['dri_numeric'] * df['comorbidity_score']\n",
    "    df['dri_karnofsky'] = df['dri_numeric'] / (df['karnofsky_score'] + 1)\n",
    "    \n",
    "    # Create categorical combinations\n",
    "    df['dri_disease_status'] = (df['dri_score'].astype(str) + '_' + \n",
    "                               df['tbi_status'].astype(str)).astype('category')\n",
    "    \n",
    "    # Ethnicity\n",
    "    df['ethnicity'].fillna(\"Missing\")\n",
    "    df['age_bin_ethnicity'] = (df['age_bin'].astype(str) + '_' + \n",
    "                    df['ethnicity'].astype(str)).astype('category')\n",
    "    df['dri_ethnicity'] = (df['dri_score'].astype(str) + '_' + \n",
    "                    df['ethnicity'].astype(str)).astype('category')\n",
    "    df['ethnicity_vivo'] = (df['ethnicity'].astype(str) + '_' + \n",
    "                    df['in_vivo_tcd'].astype(str)).astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vivo_age_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dri_disease_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vivo_prim_disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrixn after feature engineering\n",
    "def plot_correlation_matrix(df, output_path):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        plt.figure(figsize=(50, 48))\n",
    "        correlation_matrix = df[numerical_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Matrix After Feature Engineering')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/correlation_matrix_feature_engineering.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "plot_correlation_matrix(train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "#print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "for c in FEATURES:\n",
    "\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \",end=\"\")\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "        \n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "    \n",
    "train = combined.iloc[:len(train)].copy()\n",
    "test = combined.iloc[len(train):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in CATS:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "for col in CATS:\n",
    "    # Ensure categories are coded as integers starting from 0\n",
    "    train[col] = train[col].cat.codes\n",
    "    test[col] = test[col].cat.codes\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "    \n",
    "    # Verify the encoding\n",
    "    print(f\"\\nAfter fixing {col}:\")\n",
    "    print(train[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Two Targets into One Target with KaplanMeier\n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "train[\"y_km\"] = transform_survival_probability(train, time_col='efs_time', event_col='efs')\n",
    "\n",
    "plt.hist(train.loc[train.efs==1,\"y_km\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train.loc[train.efs==0,\"y_km\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Transformed Target y_km\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"KaplanMeier Transformed Target y using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_path}kaplanmeier_transformed_target_y.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cox fitter\n",
    "def transform_survival_probability(df):\n",
    "    df = df.copy()\n",
    "    df['efs_time'] = pd.to_numeric(df['efs_time'], errors='coerce')\n",
    "    df['efs'] = pd.to_numeric(df['efs'], errors='coerce')\n",
    "    required_cols = ['efs_time', 'efs'] + CATS\n",
    "    df = df.dropna(subset=['efs_time', 'efs'])\n",
    "    cph = CoxPHFitter(penalizer=0.1)\n",
    "    cph.fit(df[required_cols], duration_col='efs_time', event_col='efs')\n",
    "    y = cph.predict_partial_hazard(df)\n",
    "    return y\n",
    "train[\"y_cox\"] = transform_survival_probability(train)\n",
    "\n",
    "plt.hist(train.loc[train.efs==1,\"y_cox\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train.loc[train.efs==0,\"y_cox\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Transformed Target y_cox\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cox Transformed Target y_cox using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_path}cox_transformed_target_y.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NelsonAalenFitter\n",
    "def transform_survival_probability(df):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df['efs_time'], event_observed=df['efs'])\n",
    "    y = naf.cumulative_hazard_at_times(df['efs_time']).values\n",
    "    y = y * -1\n",
    "    return y\n",
    "train[\"y_na\"] = transform_survival_probability(train)\n",
    "\n",
    "plt.hist(train.loc[train.efs==1,\"y_km\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train.loc[train.efs==0,\"y_km\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlabel(\"Transformed Target y_na\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Nelson Aalen Transformed Target y using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_path}nelsonaalen_transformed_target_y.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9365\n",
    "\n",
    "def perform_pca(train, test, n_components=None, random_state=42):\n",
    "    # Remove rows with NaN values from both datasets\n",
    "    train = train.dropna()\n",
    "    test = test.dropna()\n",
    "\n",
    "    pca = PCA(n_components=n_components, random_state=random_state)\n",
    "    train_pca = pca.fit_transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "    \n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(f\"Explained variance ratio of the components:\\n {explained_variance_ratio}\")\n",
    "    print(np.sum(explained_variance_ratio))\n",
    "    \n",
    "    train_pca_df = pd.DataFrame(train_pca, columns=[f'PC_{i+1}' for i in range(train_pca.shape[1])])\n",
    "    test_pca_df = pd.DataFrame(test_pca, columns=[f'PC_{i+1}' for i in range(test_pca.shape[1])])\n",
    "    \n",
    "    return train_pca_df, test_pca_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMV = [\"ID\",\"efs\",\"efs_time\",\"y_cox\", \"y_km\", \"y_na\"]\n",
    "\n",
    "# PCA \n",
    "# Extract the numerical columns to be used in the PCA\n",
    "train_num = train.drop(RMV, axis=1)\n",
    "test_num = test.drop('ID', axis=1)\n",
    "\n",
    "# Get numeric and categorical columns\n",
    "numeric_columns = train_num.select_dtypes(include=['int32', 'float32']).columns\n",
    "categorical_columns = train_num.select_dtypes(exclude=['int32', 'float32']).columns\n",
    "\n",
    "# Split into numeric and categorical dataframes\n",
    "train_numeric = train_num[numeric_columns]\n",
    "test_numeric = test_num[numeric_columns]\n",
    "train_categorical = train[categorical_columns]\n",
    "test_categorical = test[categorical_columns]\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_numeric),\n",
    "    columns=train_numeric.columns\n",
    ")\n",
    "test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_numeric),\n",
    "    columns=test_numeric.columns\n",
    ")\n",
    "\n",
    "train_pca, test_pca, pca = perform_pca(train_scaled, test_scaled, n_components=15, random_state=SEED)\n",
    "\n",
    "train_rmv = train[RMV]\n",
    "\n",
    "# Merge scaled numeric data with categorical data\n",
    "train_final = pd.concat([train_scaled, train_categorical, train_pca, train_rmv], axis=1)\n",
    "test_final = pd.concat([test_scaled, test_categorical, test_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_final\n",
    "train = train_final\n",
    "\n",
    "FEATURES = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with KaplanMeier\n",
    "print(\"Using XGBoost version\",xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    #'hla_match_c_high',\n",
    "    'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    #'hla_low_res_6',\n",
    "    'graft_type',\n",
    "    'vent_hist',\n",
    "    'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    #'hla_high_res_10',\n",
    "    #'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    #'rituximab',\n",
    "    #'hla_match_drb1_low',\n",
    "    #'hla_match_dqb1_low',\n",
    "    'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    #'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    #'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    'hla_match_b_low',\n",
    "    'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    'rheum_issue',\n",
    "    'sex_match',\n",
    "    #'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    'hepatic_mild',\n",
    "    #'tce_div_match',\n",
    "    'donor_related',\n",
    "    #'melphalan_dose',\n",
    "    #'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    #'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    #'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    #'hla_match_drb1_mean',\n",
    "    #'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    #'drb1_dqb1_ratio',\n",
    "    #'high_low_diff',\n",
    "    'drb1_dqb1_diff',\n",
    "    #'hla_mean',\n",
    "    'hla_std',\n",
    "    #'hla_max',\n",
    "    #'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    #'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    'has_FK',\n",
    "    'has_MMF',\n",
    "    #'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    #'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    #'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    'dri_numeric',\n",
    "    'is_high_risk',\n",
    "    'is_standard_risk',\n",
    "    'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_xgb_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 0.9),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 600, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 20, 120),\n",
    "            'device': 'cuda',\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "        \n",
    "        model = XGBRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['y_km'],\n",
    "            eval_set=[(valid_data[features], valid_data['y_km'])],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, XGBRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_xgb: Predictions for test set\n",
    "            - oof_xgb: Out-of-fold predictions for training set\n",
    "            - xgb_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_xgb = np.zeros(len(train))\n",
    "        pred_xgb = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            # Perform hyperparameter tuning on first fold\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            # Create proper DataFrame splits for tuning\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(trial, train_fold, valid_fold, features),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'max_depth': 6,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.8,\n",
    "                    'n_estimators': 2000,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'min_child_weight': 80,\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['y_km']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['y_km']\n",
    "            x_test = test[features]\n",
    "\n",
    "            model_params = {\n",
    "                **self.best_params,\n",
    "                'device': 'cuda',\n",
    "                'enable_categorical': True\n",
    "            }\n",
    "            \n",
    "            model = XGBRegressor(**model_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=500\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_xgb[valid_idx] = model.predict(x_valid)\n",
    "            pred_xgb += model.predict(x_test)\n",
    "\n",
    "        pred_xgb /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_xgb\n",
    "        xgb_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {xgb_score}\")\n",
    "        \n",
    "        return pred_xgb, oof_xgb, xgb_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_model = XGBoostModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_xgb, oof_xgb, xgb_score, model_xgb = xgb_model.train_and_predict(\n",
    "    train, test, FEATURES, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_xgb.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES,  \n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"XGBoost KaplanMeier Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}xgboost_km_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost with KaplanMeier\n",
    "print(\"Using CatBoost version\",cb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    #'hla_match_c_high',\n",
    "    #'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    #'hla_low_res_6',\n",
    "    #'graft_type',\n",
    "    #'vent_hist',\n",
    "    #'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    #'hla_high_res_10',\n",
    "    #'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    #'rituximab',\n",
    "    #'hla_match_drb1_low',\n",
    "    #'hla_match_dqb1_low',\n",
    "    #'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    #'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    #'hla_match_b_low',\n",
    "    #'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    #'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    #'rheum_issue',\n",
    "    'sex_match',\n",
    "    #'hla_match_b_high',\n",
    "    'race_group',\n",
    "    #'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    #'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    #'melphalan_dose',\n",
    "    #'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    #'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    #'hla_high_res_mean',\n",
    "    #'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    #'comorbidity_score_by_age_at_hct',\n",
    "    #'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    #'hla_high_low_ratio',\n",
    "    'drb1_dqb1_ratio',\n",
    "    #'high_low_diff',\n",
    "    #'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    #'hla_max',\n",
    "    #'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    #'dqb1_low_interaction',\n",
    "    #'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    #'has_FK',\n",
    "    #'has_MMF',\n",
    "    #'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    #'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    #'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    #'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    #'dri_numeric',\n",
    "    #'is_high_risk',\n",
    "    'is_standard_risk',\n",
    "    #'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity'\n",
    "    #'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_catboost_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list, cat_features: List[str]) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'task_type': 'GPU',\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'grow_policy': trial.suggest_categorical('grow_policy', ['Lossguide']),\n",
    "            'depth': trial.suggest_int('depth', 2, 8),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0.1, 10.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 600, 2000),\n",
    "            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bernoulli']),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "            'early_stopping_rounds': 50,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        \n",
    "        # Handle special parameter dependencies\n",
    "        if param['bootstrap_type'] == 'Bayesian':\n",
    "            param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 10.0)\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['y_km'],\n",
    "            eval_set=(valid_data[features], valid_data['y_km']),\n",
    "            cat_features=cat_features,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        cat_features: List[str],\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, CatBoostRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            cat_features: List of categorical feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_cat: Predictions for test set\n",
    "            - oof_cat: Out-of-fold predictions for training set\n",
    "            - cat_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_cat = np.zeros(len(train))\n",
    "        pred_cat = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            # Perform hyperparameter tuning on first fold\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            # Create proper DataFrame splits for tuning\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(\n",
    "                    trial, train_fold, valid_fold, features, cat_features\n",
    "                ),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            # Add fixed parameters\n",
    "            self.best_params.update({\n",
    "                'task_type': 'GPU',\n",
    "                'early_stopping_rounds': 50\n",
    "            })\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'task_type': 'GPU',\n",
    "                    'learning_rate': 0.01,\n",
    "                    'grow_policy': 'Lossguide',\n",
    "                    'depth': 6,\n",
    "                    'min_data_in_leaf': 1,\n",
    "                    'l2_leaf_reg': 3.0,\n",
    "                    'random_strength': 1,\n",
    "                    'n_estimators': 2000,\n",
    "                    'bootstrap_type': 'Bernoulli',\n",
    "                    'subsample': 0.8,\n",
    "                    'early_stopping_rounds': 50\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['y_km']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['y_km']\n",
    "            x_test = test[features]\n",
    "            \n",
    "            model = CatBoostRegressor(**self.best_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=(x_valid, y_valid),\n",
    "                cat_features=cat_features,\n",
    "                verbose=250\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_cat[valid_idx] = model.predict(x_valid)\n",
    "            pred_cat += model.predict(x_test)\n",
    "\n",
    "        pred_cat /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_cat\n",
    "        cat_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {cat_score}\")\n",
    "        \n",
    "        return pred_cat, oof_cat, cat_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "cat_model = CatBoostModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_cat, oof_cat, cat_score, model_cat = cat_model.train_and_predict(\n",
    "    train, test, FEATURES, CATS, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_cat.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES, \n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"CatBoost KaplanMeier Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}catboost_km_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with KaplanMeier\n",
    "print(\"Using LightGBM version\",lgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    'hla_match_c_high',\n",
    "    'hla_high_res_8',\n",
    "    #'tbi_status',\n",
    "    'arrhythmia',\n",
    "    #'hla_low_res_6',\n",
    "    #'graft_type',\n",
    "    'vent_hist',\n",
    "    #'renal_issue',\n",
    "    #'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    'hla_high_res_10',\n",
    "    #'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    #'rituximab',\n",
    "    #'hla_match_drb1_low',\n",
    "    #'hla_match_dqb1_low',\n",
    "    #'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    #'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    #'in_vivo_tcd',\n",
    "    #'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    'hla_match_b_low',\n",
    "    #'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    #'rheum_issue',\n",
    "    'sex_match',\n",
    "    'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    'melphalan_dose',\n",
    "    #'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    #'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    'drb1_dqb1_ratio',\n",
    "    #'high_low_diff',\n",
    "    'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    'hla_max',\n",
    "    #'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    #'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    #'has_FK',\n",
    "    #'has_MMF',\n",
    "    #'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    #'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    #'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    #'dri_numeric',\n",
    "    #'is_high_risk',\n",
    "    #'is_standard_risk',\n",
    "    #'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_lgb_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'device': 'gpu',\n",
    "            'gpu_use_dp': True,\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.5),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 0.8),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 600, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'objective': 'regression',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = LGBMRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['y_km'],\n",
    "            eval_set=[(valid_data[features], valid_data['y_km'])]\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, LGBMRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_lgb: Predictions for test set\n",
    "            - oof_lgb: Out-of-fold predictions for training set\n",
    "            - lgb_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_lgb = np.zeros(len(train))\n",
    "        pred_lgb = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            # Perform hyperparameter tuning on first fold\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            # Create proper DataFrame splits for tuning\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(trial, train_fold, valid_fold, features),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            # Add fixed parameters\n",
    "            self.best_params.update({\n",
    "                'device': 'gpu',\n",
    "                'gpu_use_dp': True,\n",
    "                'objective': 'regression'\n",
    "            })\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'device': 'gpu',\n",
    "                    'num_leaves': 31,\n",
    "                    'max_depth': 3,\n",
    "                    'colsample_bytree': 0.4,\n",
    "                    'subsample': 0.8,\n",
    "                    'n_estimators': 2500,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'objective': 'regression',\n",
    "                    'gpu_use_dp': True\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['y_km']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['y_km']\n",
    "            x_test = test[features]\n",
    "            \n",
    "            model = LGBMRegressor(**self.best_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)]\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_lgb[valid_idx] = model.predict(x_valid)\n",
    "            pred_lgb += model.predict(x_test)\n",
    "\n",
    "        pred_lgb /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_lgb\n",
    "        lgb_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {lgb_score}\")\n",
    "        \n",
    "        return pred_lgb, oof_lgb, lgb_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lgb_model = LightGBMModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_lgb, oof_lgb, lgb_score, model_lgb = lgb_model.train_and_predict(\n",
    "    train, test, FEATURES, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_lgb.feature_importances_ \n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES,\n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color='skyblue')\n",
    "plt.xlabel(\"Importance (Gain)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"LightGBM KaplanMeier Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}lightgbm_km_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with Survival:Cox\n",
    "# SURVIVAL COX NEEDS THIS TARGET (TO DIGEST EFS AND EFS_TIME)\n",
    "train[\"efs_time2\"] = train.efs_time.copy()\n",
    "train.loc[train.efs==0,\"efs_time2\"] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    'hla_match_c_high',\n",
    "    #'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    'hla_low_res_6',\n",
    "    'graft_type',\n",
    "    'vent_hist',\n",
    "    #'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    'hla_high_res_10',\n",
    "    'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    #'rituximab',\n",
    "    'hla_match_drb1_low',\n",
    "    'hla_match_dqb1_low',\n",
    "    'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    'hla_match_b_low',\n",
    "    'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    #'rheum_issue',\n",
    "    'sex_match',\n",
    "    'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    'melphalan_dose',\n",
    "    'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    'drb1_dqb1_ratio',\n",
    "    'high_low_diff',\n",
    "    'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    'hla_max',\n",
    "    #'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    'has_FK',\n",
    "    'has_MMF',\n",
    "    #'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    #'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    'dri_numeric',\n",
    "    'is_high_risk',\n",
    "    #'is_standard_risk',\n",
    "    'is_special_case',\n",
    "    #'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostCoxModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_xgb_cox_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'device': 'cuda',\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.7),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 40, 120),\n",
    "            'enable_categorical': True,\n",
    "            'objective': 'survival:cox',\n",
    "            'eval_metric': 'cox-nloglik'\n",
    "        }\n",
    "        \n",
    "        model = XGBRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['efs_time2'],\n",
    "            eval_set=[(valid_data[features], valid_data['efs_time2'])],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, XGBRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_xgb_cox: Predictions for test set\n",
    "            - oof_xgb_cox: Out-of-fold predictions for training set\n",
    "            - xgb_cox_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_xgb_cox = np.zeros(len(train))\n",
    "        pred_xgb_cox = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(trial, train_fold, valid_fold, features),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            self.best_params.update({\n",
    "                'device': 'cuda',\n",
    "                'enable_categorical': True,\n",
    "                'objective': 'survival:cox',\n",
    "                'eval_metric': 'cox-nloglik'\n",
    "            })\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'device': 'cuda',\n",
    "                    'max_depth': 3,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.8,\n",
    "                    'n_estimators': 2000,\n",
    "                    'learning_rate': 0.02,\n",
    "                    'enable_categorical': True,\n",
    "                    'min_child_weight': 80,\n",
    "                    'objective': 'survival:cox',\n",
    "                    'eval_metric': 'cox-nloglik'\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['efs_time2']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['efs_time2']\n",
    "            x_test = test[features]\n",
    "            \n",
    "            model = XGBRegressor(**self.best_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=500\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_xgb_cox[valid_idx] = model.predict(x_valid)\n",
    "            pred_xgb_cox += model.predict(x_test)\n",
    "\n",
    "        pred_xgb_cox /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_xgb_cox\n",
    "        xgb_cox_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {xgb_cox_score}\")\n",
    "        \n",
    "        return pred_xgb_cox, oof_xgb_cox, xgb_cox_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_cox_model = XGBoostCoxModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_xgb_cox, oof_xgb_cox, xgb_cox_score, model_xgb_cox = xgb_cox_model.train_and_predict(\n",
    "    train, test, FEATURES, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_xgb_cox.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES,  # Replace FEATURES with your list of feature names\n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"XGBoost Survival:Cox Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}xgboost_cox_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost with Survival:Cox\n",
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    #'hla_match_c_high',\n",
    "    #'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    #'hla_low_res_6',\n",
    "    'graft_type',\n",
    "    'vent_hist',\n",
    "    #'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    #'hla_high_res_10',\n",
    "    'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    'rituximab',\n",
    "    #'hla_match_drb1_low',\n",
    "    #'hla_match_dqb1_low',\n",
    "    'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    #'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    #'hla_match_b_low',\n",
    "    'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    #'rheum_issue',\n",
    "    'sex_match',\n",
    "    #'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    #'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    #'melphalan_dose',\n",
    "    #'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    #'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    #'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    #'drb1_dqb1_ratio',\n",
    "    #'high_low_diff',\n",
    "    #'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    'hla_max',\n",
    "    #'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    'has_FK',\n",
    "    #'has_MMF',\n",
    "    #'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    #'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    #'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    #'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    'dri_numeric',\n",
    "    'is_high_risk',\n",
    "    #'is_standard_risk',\n",
    "    #'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostCoxModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_catboost_cox_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list, cat_features: List[str]) -> float:\n",
    "        param = {\n",
    "            'loss_function': 'Cox',\n",
    "            'iterations': trial.suggest_int('iterations', 400, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'grow_policy': trial.suggest_categorical('grow_policy', ['Lossguide']),\n",
    "            'depth': trial.suggest_int('depth', 2, 9),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 4.0, 10.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 80),\n",
    "            'rsm': trial.suggest_float('rsm', 0.1, 0.3),\n",
    "            'use_best_model': False,\n",
    "            'early_stopping_rounds': 75\n",
    "        }\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['efs_time2'],\n",
    "            eval_set=(valid_data[features], valid_data['efs_time2']),\n",
    "            cat_features=cat_features,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        cat_features: List[str],\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, CatBoostRegressor]:\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_cat_cox = np.zeros(len(train))\n",
    "        pred_cat_cox = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(\n",
    "                    trial, train_fold, valid_fold, features, cat_features\n",
    "                ),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            self.best_params.update({\n",
    "                'loss_function': 'Cox',\n",
    "                'use_best_model': False,\n",
    "                'early_stopping_rounds': 100\n",
    "            })\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                self.best_params = {\n",
    "                    'loss_function': 'Cox',\n",
    "                    'iterations': 600,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'grow_policy': 'Lossguide',\n",
    "                    'use_best_model': False,\n",
    "                    'early_stopping_rounds': 100,\n",
    "                    'depth': 4,\n",
    "                    'l2_leaf_reg': 5.0\n",
    "                }\n",
    "\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['efs_time2']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['efs_time2']\n",
    "            x_test = test[features]\n",
    "            \n",
    "            model = CatBoostRegressor(**self.best_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=(x_valid, y_valid),\n",
    "                cat_features=cat_features,\n",
    "                verbose=100\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_cat_cox[valid_idx] = model.predict(x_valid)\n",
    "            pred_cat_cox += model.predict(x_test)\n",
    "\n",
    "        pred_cat_cox /= self.n_folds\n",
    "        \n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_cat_cox\n",
    "        cat_cox_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {cat_cox_score}\")\n",
    "        \n",
    "        return pred_cat_cox, oof_cat_cox, cat_cox_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "cat_cox_model = CatBoostCoxModel(n_folds=10, n_trials=200)\n",
    "\n",
    "pred_cat_cox, oof_cat_cox, cat_cox_score, model_cat_cox = cat_cox_model.train_and_predict(\n",
    "    train, test, FEATURES, CATS, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_cat_cox.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES, \n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"CatBoost Survival:Cox Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}catboost_cox_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with Cox\n",
    "print(\"Using LightGBM version\",lgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    'hla_match_c_high',\n",
    "    'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    'hla_low_res_6',\n",
    "    'graft_type',\n",
    "    'vent_hist',\n",
    "    #'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    #'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    'hla_high_res_10',\n",
    "    'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    #'hla_match_c_low',\n",
    "    'rituximab',\n",
    "    'hla_match_drb1_low',\n",
    "    'hla_match_dqb1_low',\n",
    "    'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    #'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    #'prior_tumor',\n",
    "    'hla_match_b_low',\n",
    "    #'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    #'rheum_issue',\n",
    "    'sex_match',\n",
    "    'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    #'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    'melphalan_dose',\n",
    "    'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    'drb1_dqb1_ratio',\n",
    "    'high_low_diff',\n",
    "    'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    'hla_max',\n",
    "    'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    'with_tbi',\n",
    "    'sex_donor',\n",
    "    'sex_recipient',\n",
    "    'has_FK',\n",
    "    'has_MMF',\n",
    "    'has_MTX',\n",
    "    #'has_CSA',\n",
    "    #'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    'n_agents',\n",
    "    #'is_complex',\n",
    "    #'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    'karnofsky_has_csa',\n",
    "    #'karnofsky_is_depletion_based',\n",
    "    #'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    #'dri_numeric',\n",
    "    #'is_high_risk',\n",
    "    #'is_standard_risk',\n",
    "    #'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_lgb_cox_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'device': 'gpu',\n",
    "            'gpu_use_dp': True,\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 25, 63),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 9),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.4),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 0.8),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 400, 1400),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 40, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.1),\n",
    "            'objective': 'regression',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = LGBMRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['efs_time2'],\n",
    "            eval_set=[(valid_data[features], valid_data['efs_time2'])]\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, LGBMRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_lgb: Predictions for test set\n",
    "            - oof_lgb: Out-of-fold predictions for training set\n",
    "            - lgb_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_lgb_cox = np.zeros(len(train))\n",
    "        pred_lgb_cox = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            # Perform hyperparameter tuning on first fold\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            # Create proper DataFrame splits for tuning\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(trial, train_fold, valid_fold, features),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            # Add fixed parameters\n",
    "            self.best_params.update({\n",
    "                'device': 'gpu',\n",
    "                'gpu_use_dp': True,\n",
    "                'objective': 'regression'\n",
    "            })\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'device': 'gpu',\n",
    "                    'num_leaves': 31,\n",
    "                    'max_depth': 3,\n",
    "                    'colsample_bytree': 0.4,\n",
    "                    'subsample': 0.8,\n",
    "                    'n_estimators': 2500,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'objective': 'regression',\n",
    "                    'gpu_use_dp': True\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['efs_time2']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['efs_time2']\n",
    "            x_test = test[features]\n",
    "            \n",
    "            model = LGBMRegressor(**self.best_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)]\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_lgb_cox[valid_idx] = model.predict(x_valid)\n",
    "            pred_lgb_cox += model.predict(x_test)\n",
    "\n",
    "        pred_lgb_cox /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_lgb_cox\n",
    "        lgb_cox_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {lgb_cox_score}\")\n",
    "        \n",
    "        return pred_lgb_cox, oof_lgb_cox, lgb_cox_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lgb_cox_model = LightGBMModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_lgb_cox, oof_lgb_cox, lgb_cox_score, model_lgb_cox = lgb_cox_model.train_and_predict(\n",
    "    train, test, FEATURES, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_lgb_cox.feature_importances_ \n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES,\n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color='skyblue')\n",
    "plt.xlabel(\"Importance (Gain)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"LightGBM KaplanMeier Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}lightgbm_km_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with Nelson Aalen\n",
    "print(\"Using XGBoost version\",xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "FEATURES = ['dri_score',\n",
    "    'psych_disturb',\n",
    "    'cyto_score',\n",
    "    'diabetes',\n",
    "    'hla_match_c_high',\n",
    "    'hla_high_res_8',\n",
    "    'tbi_status',\n",
    "    'arrhythmia',\n",
    "    'hla_low_res_6',\n",
    "    'graft_type',\n",
    "    'vent_hist',\n",
    "    'renal_issue',\n",
    "    'pulm_severe',\n",
    "    'prim_disease_hct',\n",
    "    'hla_high_res_6',\n",
    "    'cmv_status',\n",
    "    'hla_high_res_10',\n",
    "    'hla_match_dqb1_high',\n",
    "    'tce_imm_match',\n",
    "    'hla_nmdp_6',\n",
    "    'hla_match_c_low',\n",
    "    'rituximab',\n",
    "    'hla_match_drb1_low',\n",
    "    'hla_match_dqb1_low',\n",
    "    'prod_type',\n",
    "    'cyto_score_detail',\n",
    "    'conditioning_intensity',\n",
    "    'ethnicity',\n",
    "    'year_hct',\n",
    "    'obesity',\n",
    "    'mrd_hct',\n",
    "    'in_vivo_tcd',\n",
    "    'tce_match',\n",
    "    'hla_match_a_high',\n",
    "    'hepatic_severe',\n",
    "    'donor_age',\n",
    "    'prior_tumor',\n",
    "    'hla_match_b_low',\n",
    "    'peptic_ulcer',\n",
    "    'age_at_hct',\n",
    "    'hla_match_a_low',\n",
    "    'gvhd_proph',\n",
    "    'rheum_issue',\n",
    "    'sex_match',\n",
    "    'hla_match_b_high',\n",
    "    'race_group',\n",
    "    'comorbidity_score',\n",
    "    'karnofsky_score',\n",
    "    'hepatic_mild',\n",
    "    'tce_div_match',\n",
    "    'donor_related',\n",
    "    'melphalan_dose',\n",
    "    'hla_low_res_8',\n",
    "    'cardiac',\n",
    "    'hla_match_drb1_high',\n",
    "    'pulm_moderate',\n",
    "    'hla_low_res_10',\n",
    "    #'PC_1',\n",
    "    #'PC_2',\n",
    "    #'PC_3',\n",
    "    #'PC_4',\n",
    "    #'PC_5',\n",
    "    #'PC_6',\n",
    "    #'PC_7',\n",
    "    #'PC_8',\n",
    "    #'PC_9',\n",
    "    #'PC_10',\n",
    "    #'PC_11',\n",
    "    #'PC_12',\n",
    "    #'PC_13',\n",
    "    #'PC_14',\n",
    "    #'PC_15',\n",
    "    'comorbidity_age',\n",
    "    'age_bin',\n",
    "    #'cyto_age',\n",
    "    'graft_prod',\n",
    "    'age_bin_race',\n",
    "    'age_bin_pulm_severe',\n",
    "    'hla_high_res_mean',\n",
    "    'hla_low_res_mean',\n",
    "    'hla_ratio_res_highlow',\n",
    "    'hla_ratio_res_lowhigh',\n",
    "    'age_bin_dri',\n",
    "    'donor_by_age_at_hct',\n",
    "    'comorbidity_score_by_age_at_hct',\n",
    "    'hla_match_drb1_mean',\n",
    "    'hla_match_dqb1_mean',\n",
    "    'hla_high_low_ratio',\n",
    "    'drb1_dqb1_ratio',\n",
    "    'high_low_diff',\n",
    "    'drb1_dqb1_diff',\n",
    "    'hla_mean',\n",
    "    'hla_std',\n",
    "    'hla_max',\n",
    "    'hla_min',\n",
    "    'drb1_high_interaction',\n",
    "    'dqb1_low_interaction',\n",
    "    'with_tbi',\n",
    "    #'sex_donor',\n",
    "    #'sex_recipient',\n",
    "    'has_FK',\n",
    "    'has_MMF',\n",
    "    'has_MTX',\n",
    "    'has_CSA',\n",
    "    'has_cyclophosphamide',\n",
    "    #'has_combination',\n",
    "    #'n_agents',\n",
    "    #'is_complex',\n",
    "    'is_depletion_based',\n",
    "    #'is_monotherapy',\n",
    "    #'no_prophylaxis',\n",
    "    'primary_agent',\n",
    "    'is_standard_approach',\n",
    "    #'is_experimental',\n",
    "    #'FK_MMF_interaction',\n",
    "    #'CSA_MTX_interaction',\n",
    "    'karnofsky_donor_comorbidity_age',\n",
    "    'karnofsky_has_csa',\n",
    "    'karnofsky_is_depletion_based',\n",
    "    'karnofsky_is_monotherapy',\n",
    "    'karnofsky_age_at_hct',\n",
    "    'karnofsky_comorbidity_score',\n",
    "    'comorbidity_score_karnofsky',\n",
    "    'vivo_age_bin',\n",
    "    'vivo_comorbidity',\n",
    "    'vivo_prim_disease',\n",
    "    'dri_numeric',\n",
    "    'is_high_risk',\n",
    "    'is_standard_risk',\n",
    "    'is_special_case',\n",
    "    'dri_age',\n",
    "    'dri_comorbidity',\n",
    "    'dri_karnofsky',\n",
    "    'dri_disease_status',\n",
    "    'age_bin_ethnicity',\n",
    "    'dri_ethnicity',\n",
    "    'ethnicity_vivo'\n",
    "    ]\n",
    "\n",
    "# Get categorical columns from train features\n",
    "CATS = train[FEATURES].select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        random_state: int = 42,\n",
    "        n_trials: int = 50,\n",
    "        best_params_path: str = 'best_xgb_na_params.json'\n",
    "    ):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params_path = best_params_path\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_data: pd.DataFrame, valid_data: pd.DataFrame,\n",
    "                 features: list) -> float:\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "        param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 0.9),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 600, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 20, 120),\n",
    "            'device': 'cuda',\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "        \n",
    "        model = XGBRegressor(**param)\n",
    "        model.fit(\n",
    "            train_data[features], train_data['y_na'],\n",
    "            eval_set=[(valid_data[features], valid_data['y_na'])],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        predictions = model.predict(valid_data[features])\n",
    "        \n",
    "        # Create prediction DataFrame in required format\n",
    "        y_true = valid_data[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = valid_data[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = predictions\n",
    "        \n",
    "        fold_score = score(y_true, y_pred, \"ID\")\n",
    "        return fold_score\n",
    "    \n",
    "    def save_best_params(self, params: Dict) -> None:\n",
    "        \"\"\"Save the best parameters to a JSON file.\"\"\"\n",
    "        with open(self.best_params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_best_params(self) -> Optional[Dict]:\n",
    "        \"\"\"Load the best parameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.best_params_path):\n",
    "            with open(self.best_params_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def train_and_predict(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "        features: list,\n",
    "        tune_hyperparameters: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, XGBRegressor]:\n",
    "        \"\"\"\n",
    "        Train the model and make predictions, with optional hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            train: Training DataFrame\n",
    "            test: Test DataFrame\n",
    "            features: List of feature columns\n",
    "            tune_hyperparameters: Whether to perform hyperparameter tuning\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - pred_xgb: Predictions for test set\n",
    "            - oof_xgb: Out-of-fold predictions for training set\n",
    "            - xgb_score: Model score\n",
    "            - best_model: Best trained model\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        oof_xgb_na = np.zeros(len(train))\n",
    "        pred_xgb_na = np.zeros(len(test))\n",
    "        best_model = None\n",
    "\n",
    "        if tune_hyperparameters:\n",
    "            # Perform hyperparameter tuning on first fold\n",
    "            print(\"Starting hyperparameter tuning...\")\n",
    "            train_idx, valid_idx = next(kf.split(train))\n",
    "            \n",
    "            # Create proper DataFrame splits for tuning\n",
    "            train_fold = train.iloc[train_idx].copy()\n",
    "            valid_fold = train.iloc[valid_idx].copy()\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(\n",
    "                lambda trial: self.objective(trial, train_fold, valid_fold, features),\n",
    "                n_trials=self.n_trials\n",
    "            )\n",
    "            \n",
    "            self.best_params = study.best_params\n",
    "            self.save_best_params(self.best_params)\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        else:\n",
    "            self.best_params = self.load_best_params()\n",
    "            if self.best_params is None:\n",
    "                print(\"No saved parameters found. Using default parameters.\")\n",
    "                self.best_params = {\n",
    "                    'max_depth': 6,\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample': 0.8,\n",
    "                    'n_estimators': 2000,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'min_child_weight': 80,\n",
    "                }\n",
    "\n",
    "        # Train the model with best parameters\n",
    "        for i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n",
    "            print(f\"Training fold {i+1}/{self.n_folds}\")\n",
    "            \n",
    "            x_train = train.iloc[train_idx][features]\n",
    "            y_train = train.iloc[train_idx]['y_km']\n",
    "            x_valid = train.iloc[valid_idx][features]\n",
    "            y_valid = train.iloc[valid_idx]['y_km']\n",
    "            x_test = test[features]\n",
    "\n",
    "            model_params = {\n",
    "                **self.best_params,\n",
    "                'device': 'cuda',\n",
    "                'enable_categorical': True\n",
    "            }\n",
    "            \n",
    "            model = XGBRegressor(**model_params)\n",
    "            model.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=500\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                best_model = model\n",
    "\n",
    "            oof_xgb_na[valid_idx] = model.predict(x_valid)\n",
    "            pred_xgb_na += model.predict(x_test)\n",
    "\n",
    "        pred_xgb_na /= self.n_folds\n",
    "        \n",
    "        # Calculate final score\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_xgb_na\n",
    "        xgb_na_score = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "        print(f\"\\nOverall CV Score: {xgb_na_score}\")\n",
    "        \n",
    "        return pred_xgb_na, oof_xgb_na, xgb_na_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_na_model = XGBoostModel(n_folds=10, n_trials=200)\n",
    "\n",
    "# For hyperparameter tuning:\n",
    "pred_xgb_na, oof_xgb_na, xgb_na_score, model_xgb_na = xgb_na_model.train_and_predict(\n",
    "    train, test, FEATURES, tune_hyperparameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model_xgb_na.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": FEATURES,  \n",
    "    \"Importance\": feature_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"XGBoost Nelson Aalen Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "plt.savefig(f'{output_path}xgboost_na_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nxgb score =\",xgb_score)\n",
    "print(f\"\\ncat score =\",cat_score)\n",
    "print(f\"\\nlgb score =\",lgb_score)\n",
    "print(f\"\\nxgb cox score =\",xgb_cox_score)\n",
    "print(f\"\\ncat cox score =\",cat_cox_score)\n",
    "print(f\"\\nlgb cox score =\",lgb_cox_score)\n",
    "print(f\"\\nxgb na score =\",xgb_na_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lgb_cox, xgb_na, cat_na, lgb_na\n",
    "oof_preds = [\n",
    "    oof_xgb, #km\n",
    "    oof_cat, #km\n",
    "    oof_lgb, #km\n",
    "    oof_xgb_cox, #cox efstime2\n",
    "    oof_cat_cox, #cox efstime2\n",
    "    oof_lgb_cox, #cox efstime2\n",
    "    oof_xgb_na #na\n",
    "]\n",
    "\n",
    "weights = [0.3, 0.5, 0.2, 0.4, 0.4, 0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_oof_preds = np.sum([w * p for w, p in zip(weights, ranked_oof_preds)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble CAT and XGB and LGB\n",
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = ensemble_oof_preds\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for Ensemble =\",m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    pred_xgb,\n",
    "    pred_cat,\n",
    "    pred_lgb,\n",
    "    pred_xgb_cox,\n",
    "    pred_cat_cox,\n",
    "    pred_lgb_cox,\n",
    "    pred_xgb_na\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_preds = np.array([rankdata(p) for p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = np.sum([w * p for w, p in zip(weights, ranked_preds)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"input/data/sample_submission.csv\")\n",
    "\n",
    "# Print individual model rankings with np.set_printoptions for full output\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Combine rankings\n",
    "sub.prediction = ensemble_preds\n",
    "\n",
    "print(\"\\nFinal ensemble scores:\")\n",
    "print(sub.prediction)\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSub shape:\", sub.shape)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
